# Personal Earnings Tracker - Development Specification

## Project Overview
Build a personal web application to track stock earnings data by combining historical EPS from SEC APIs with current earnings estimates and timing from investor.com scraping. Vercel project should be hosted in the "earnings-tracker" folder.

## Tech Stack
- **Frontend/Backend**: Vercel (Next.js recommended) âœ…
- **Database**: Supabase âœ…
- **Automation**: GitHub Actions for scheduled scraping âœ…
- **APIs**: SEC EDGAR API for historical data âœ…

## Core Features

### 1. User Authentication âœ…
- **Supabase Auth**: Email/password login system âœ…
- **Protected Routes**: Secure all data views behind authentication âœ…
- **User Sessions**: Persistent login with proper session management âœ…

### 2. Historical EPS Data Collection âœ…
- **Source**: SEC EDGAR API âœ…
- **Data Points**:
  - Company ticker symbol âœ…
  - Historical quarterly EPS (actual reported) âœ…
  - Filing dates âœ…
  - Fiscal periods âœ…
- **Implementation Notes**:
  - SEC API requires User-Agent header with contact info âœ…
  - Rate limit: 10 requests/second max âœ…
  - Store data efficiently (SEC provides bulk options) âœ…

### 3. Current Earnings Data Scraping ğŸš§
- **Source**: Investor.com (or similar financial sites) âœ…
- **Data Points**:
  - Upcoming earnings dates âœ…
  - Before/after market timing âœ…
  - EPS estimates (analyst consensus) âœ…
  - Company ticker symbols âœ…
- **Automation**: GitHub Actions workflow âœ…
- **Frequency**: Daily scraping recommended âœ…
- **Status**: Scraping infrastructure complete, actual scraping logic needs implementation

### 4. Data Grid Interface âœ…
- **Framework**: Custom React component âœ…
- **Features**:
  - **Sorting**: Multi-column sorting capability âœ…
  - **Filtering**: Column-specific filters (date ranges, text search, number ranges) âœ…
  - **Search**: Global search across all visible columns âœ…
  - **Pagination**: Handle large datasets efficiently âœ…
  - **Export**: CSV export functionality âœ…
- **Columns**: Ticker, Company Name, Earnings Date, Market Timing, EPS Estimate, Historical EPS, etc. âœ…

### 5. Watchlist Management âœ…
- **Multiple Lists**: Users can create multiple named watchlists âœ…
- **Stock Management**: Add/remove stocks from watchlists âœ…
- **List Operations**: Create, rename, delete watchlists âœ…
- **Quick Actions**: Add/remove implemented (drag-and-drop pending)
- **Persistence**: All watchlists saved to user's account âœ…

### 6. Web Interface âœ…
- **Purpose**: Personal dashboard to view/analyze earnings data âœ…
- **Authentication**: Supabase Auth with email/password login âœ…
- **Key Views**:
  - Upcoming earnings calendar âœ…
  - Historical vs estimated EPS comparison âœ…
  - Stock-specific earnings history âœ…
  - Grid view with filtering, sorting, and search capabilities âœ…
  - Saved stock watchlists âœ…

## Database Schema (Supabase) âœ…

### `users` table (Supabase Auth) âœ…
```sql
-- Handled automatically by Supabase Auth
-- Contains: id, email, created_at, etc.
```

### `companies` table âœ…
```sql
- id (uuid, primary key)
- ticker (text, unique)
- company_name (text)
- created_at (timestamp)
```

### `user_watchlists` table âœ…
```sql
- id (uuid, primary key)
- user_id (uuid, foreign key to auth.users)
- name (text) -- e.g., "Tech Stocks", "My Favorites"
- created_at (timestamp)
- updated_at (timestamp)
```

### `watchlist_stocks` table âœ…
```sql
- id (uuid, primary key)
- watchlist_id (uuid, foreign key)
- company_id (uuid, foreign key)
- added_at (timestamp)
```

### `historical_eps` table âœ…
```sql
- id (uuid, primary key)
- company_id (uuid, foreign key)
- fiscal_period (text) -- e.g., "Q1 2024"
- eps_actual (decimal)
- filing_date (date)
- created_at (timestamp)
```

### `earnings_estimates` table âœ…
```sql
- id (uuid, primary key)
- company_id (uuid, foreign key)
- earnings_date (date)
- market_timing (text) -- "before" or "after"
- eps_estimate (decimal)
- last_updated (timestamp)
- created_at (timestamp)
```

## GitHub Actions Workflow âœ…

### Scraping Job âœ…
- **Trigger**: Daily cron schedule âœ…
- **Steps**:
  1. Scrape investor.com for earnings calendar âœ…
  2. Parse earnings dates, timing, and estimates âœ…
  3. Update Supabase database âœ…
  4. Handle errors gracefully with retry logic âœ…

### Workflow Implementation âœ…
```yaml
name: Scrape Earnings Data
on:
  schedule:
    - cron: '0 6 * * *'  # Daily at 6 AM UTC
  workflow_dispatch:  # Manual trigger option

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Setup Node.js
      - name: Install dependencies
      - name: Run scraper
      - name: Update database
```

## Technical Considerations

### Rate Limiting & Reliability âœ…
- **SEC API**: Respect 10 req/sec limit, include proper User-Agent âœ…
- **Web Scraping**: Add delays between requests, implement retry logic âœ…
- **Error Handling**: Log failures, continue processing other stocks âœ…

### Data Management âœ…
- **Deduplication**: Prevent duplicate entries for same earnings periods âœ…
- **Data Validation**: Verify scraped data formats before database insertion âœ…
- **Storage Optimization**: Archive old estimates after earnings dates pass âœ…

### Security & Environment âœ…
- **Authentication**: Supabase Auth with email/password âœ…
- **Row Level Security**: Enable RLS on user-specific tables (watchlists) âœ…
- **API Keys**: Store Supabase credentials in GitHub Secrets âœ…
- **CORS**: Configure for Vercel domain only âœ…
- **Headers**: Include proper User-Agent for SEC compliance âœ…

## Development Phases

### Phase 1: Core Infrastructure âœ…
1. Set up Vercel + Supabase integration âœ…
2. Create database schema âœ…
3. Build basic SEC API integration âœ…
4. Simple web interface for viewing data âœ…

### Phase 2: Scraping Automation ğŸš§
1. Develop scraping logic for investor.com ğŸš§ (infrastructure done, logic pending)
2. Set up GitHub Actions workflow âœ…
3. Implement error handling and logging âœ…
4. Test automation end-to-end â³

### Phase 3: Enhanced Interface & User Features âœ…
1. Implement Supabase Auth (email/password login) âœ…
2. Build data grid component with filtering, sorting, search âœ…
3. Create watchlist management (add/remove stocks, multiple lists) âœ…
4. Add earnings calendar view âœ…
5. Implement historical vs estimate comparisons âœ…
6. Polish UI/UX with responsive design âœ…

## Deployment Checklist
- [x] Vercel project configured
- [x] Supabase database created with proper schema
- [x] Supabase Auth configured (email provider, policies)
- [x] Row Level Security (RLS) policies set up
- [x] Environment variables set (Supabase URL, API keys)
- [ ] GitHub Actions secrets configured
- [x] SEC API compliance (User-Agent header)
- [ ] Initial data seeded for testing
- [ ] Authentication flow tested (signup/login/logout)

## Current Status (January 2025)

### Completed âœ…
1. **Project Restructuring**: Removed legacy API integrations (Polygon, Alpha Vantage, Finnhub)
2. **Database Schema**: Implemented all tables with proper relationships and RLS
3. **SEC EDGAR API**: Full integration with rate limiting and data fetching
4. **Data Grid**: Advanced filtering, sorting, searching, and CSV export
5. **Watchlist Management**: Complete CRUD operations for multiple watchlists
6. **Authentication**: Supabase Auth with protected routes
7. **GitHub Actions**: Workflow configured for daily scraping
8. **API Routes**: All necessary endpoints for companies, watchlists, and data

### In Progress ğŸš§
1. **Investor.com Scraping**: Infrastructure complete, actual scraping logic needs implementation
2. **Deployment**: Ready for Vercel deployment with provided configuration

### Next Steps ğŸ“‹
1. Deploy to Vercel using the deployment guide
2. Configure GitHub Secrets for Actions
3. Implement actual investor.com scraping logic in `scripts/scrape-earnings.js`
4. Test end-to-end workflow
5. Seed initial data for testing

## Legal & Compliance Notes
- **Personal Use Only**: This is for individual use, not commercial âœ…
- **SEC Compliance**: Follow SEC EDGAR API guidelines âœ…
- **Web Scraping**: Respect robots.txt and reasonable request rates âœ…
- **Data Storage**: Only store publicly available financial data âœ…

## Success Metrics
- Historical EPS data successfully retrieved and stored âœ…
- Daily scraping runs without errors â³
- Web interface displays accurate, up-to-date earnings information âœ…
- System handles rate limits and errors gracefully âœ…

---

**Contact**: mattmass123@gmail.com (configured in SEC API)
**Priority**: Personal project, focus on reliability over features